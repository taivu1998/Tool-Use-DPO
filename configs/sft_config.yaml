model_name: "Qwen/Qwen2.5-Coder-7B-Instruct"
data_path: "data/synthetic_triplets.jsonl"
output_dir: "checkpoints/sft_cold_start"
log_file: "logs/sft_training.log"
max_seq_length: 2048
batch_size: 8              # L4 can handle 8
grad_accum_steps: 2        # Effective batch = 16
learning_rate: 3.0e-5      # Slightly higher for stronger learning
epochs: 6                  # 6 epochs for thorough schema learning
warmup_steps: 100          # ~2% of total steps
weight_decay: 0.01         # Light regularization
seed: 42